import copy

import numpy
import torch
from torch import nn
from torch.nn import functional as F
from torch.nn.utils.rnn import pad_sequence

from enet import consts
from enet.models.DynamicLSTM import DynamicLSTM
from enet.models.EmbeddingLayer import EmbeddingLayer, MultiLabelEmbeddingLayer
from enet.models.GCN import GraphConvolution
from enet.models.HighWay import HighWay
from enet.models.SelfAttention import AttentionLayer
from enet.models.model import Model
from enet.testing import EDTester
from enet.util import BottledXavierLinear
from pytorch_pretrained_bert import BertModel

class EDModel(Model):
    def __init__(self, hyps, device=torch.device("cpu"), embeddingMatrix=None):
        super(EDModel, self).__init__()
        self.hyperparams = copy.deepcopy(hyps)
        self.device = device

        # bert
        self.bert = BertModel.from_pretrained("/home/yk/.pytorch_pretrained_bert/bert-base-uncased")

        # Output Linear
        self.ol = BottledXavierLinear(in_features=4 * 768, out_features=hyps["oc"]).to(device=device)

        # AE Output Linear
        self.ae_ol = BottledXavierLinear(in_features=4 * 768, out_features=hyps["ae_oc"]).to(device=device)

        # Move to right device
        self.to(self.device)

    def get_sentence_positional_feature(self, BATCH_SIZE, SEQ_LEN):
        positions = [[abs(j) for j in range(-i, SEQ_LEN - i)] for i in range(SEQ_LEN)]  # list [SEQ_LEN, SEQ_LEN]
        positions = [torch.LongTensor(position) for position in positions]  # list of tensors [SEQ_LEN]
        positions = [torch.cat([position] * BATCH_SIZE).resize_(BATCH_SIZE, position.size(0))
                     for position in positions]  # list of tensors [BATCH_SIZE, SEQ_LEN]
        return positions

    def forward(self, tokens, words, x_len, entities, label_i2s, golden_events):
        '''
            extracting event triggers
        '''
        SEP = torch.LongTensor([102]).to(self.device)
        
        BATCH_SIZE = tokens.size()[0]
        SEQ_LEN = tokens.size()[1]
        
        batch_trigger_logits = []
        batch_trigger_prediction = []
        # 对 batch 内每个句子进行处理
        # 比较慢我也莫得办法
        for b in range(BATCH_SIZE):
            seqs = []
            lens = []
            # 对每一个句子拼凑trigger, 进行预测
            ts, ws, w_len = tokens[b], words[b], x_len[b]
            # 每个词
            for w in ws["_LIST_"]:
                # 得到token
                w_tokens = ws[w]
                wv = torch.LongTensor(w_tokens).to(self.device)
                # [CLS] sentence [SEP] trigger [SEP]
                sent_with_trigger = torch.cat([ts[:w_len], wv, SEP], 0)
                
                seqs.append(sent_with_trigger)
                lens.append(sent_with_trigger.size()[0])
            
            # 处理完毕，生成padding
            x_in = pad_sequence(seqs, batch_first=True)
            # mask
            mask = numpy.zeros(shape=x_in.size(), dtype=numpy.uint8)
            segment = numpy.zeros(shape=x_in.size(), dtype=numpy.uint8)
            for i, l in enumerate(lens):
                mask[i, 0:l] = numpy.ones(shape=(l), dtype=numpy.uint8)
            mask = torch.LongTensor(mask).to(self.device)
            segment = torch.LongTensor(segment).to(self.device)
            
            # feed into bert
            bert_encode_layers = None
            if self.training:
                self.bert.train()
                bert_encode_layers, _ = self.bert(x_in, segment, mask)
            else:
                with torch.no_grad():
                    bert_encode_layers, _ = self.bert(x_in, segment, mask)
                    


            # concat last 4 layers output
            h = torch.cat(bert_encode_layers[-4:], 2) # (x_len, 4*768)
            cls = h[:, 0]

            # trigger prediction
            trigger_logits = self.ol(cls)
            
            # trigger 预测
            trigger_output = torch.argmax(trigger_logits, -1)
            
            # 添加到 batch
            batch_trigger_logits.extend(trigger_logits)
            batch_trigger_prediction.append(trigger_output)

        # 整个 batch 的 trigger 预测结果
        batch_trigger_logits = torch.stack(batch_trigger_logits, dim=0)
        batch_trigger = pad_sequence(batch_trigger_prediction, batch_first=True, padding_value=-1)  # (batch, seq)
        
        #  对role进行预测
        #  在训练时，we should always feed gold event label to ensure that
        #            the model can really learn information from dataset rather
        #            than fake results generated by model prediction.
        # 取每个batch
        batch_ent_logits = []
        batch_ent_keys = []
        for b in range(BATCH_SIZE):
            # 句子长度
            word_dict = words[b]
            word_list = word_dict['_LIST_']
            word_len = len(word_list)
            # trigger seq
            sents = tokens[b][:x_len[b]]
            trigger_seq = batch_trigger[b][:word_len]
            # words
            # entities
            ents = entities[b]
            # golden_events
            g_events = golden_events[b]
            
            # 首先生成事件列表
            trigger_dict = {}
            if self.training:
                # 训练时传入 golden event trigger
                for e in g_events.keys():
                    trigger_dict[e[0]] = {
                        "words": word_list[e[0]],
                        "type": e[1],
                        "tokens": torch.LongTensor(word_dict[word_list[e[0]]]).to(self.device)
                    }
            else:
                trigger_index = trigger_seq.nonzero().squeeze().tolist()
                for i in trigger_index:
                    trigger_dict[i] = {
                        "words": word_list[i],
                        "type": label_i2s[trigger_seq[i]],
                        "tokens": torch.LongTensor(word_dict[word_list[i]]).to(self.device)
                    }
            
            # 对每个event进行预测 role
            ent_seqs = []
            ent_lens = []
            for i, v in trigger_dict.items():
                trigger_tokens = v["tokens"]
                # 将每个 event 与 entity 联合
                for ent, ent_v in ents.items():
                    ent_tokens = torch.LongTensor(ent_v["tokens"]).to(self.device)
                    # [CLS] sentence [SEP] trigger [SEP] entity [SEP]
                    sent_with_entity = torch.cat([sents, trigger_tokens, SEP, ent_tokens, SEP], 0)
                    
                    ent_seqs.append(sent_with_entity)
                    ent_lens.append(sent_with_entity.size()[0])
                    
                    batch_ent_keys.append({
                        "batch": b,
                        "trigger": i,
                        "trigger_type": v["type"],
                        "entity": ent,
                    })
                    
            if len(ent_seqs) == 0:
                continue
                
            # 处理完毕，生成padding
            ent_x_in = pad_sequence(ent_seqs, batch_first=True)
            # mask
            ent_mask = numpy.zeros(shape=ent_x_in.size(), dtype=numpy.uint8)
            ent_segment = numpy.zeros(shape=ent_x_in.size(), dtype=numpy.uint8)
            for i, l in enumerate(ent_lens):
                ent_mask[i, 0:l] = numpy.ones(shape=(l), dtype=numpy.uint8)
            ent_mask = torch.LongTensor(ent_mask).to(self.device)
            ent_segment = torch.LongTensor(ent_segment).to(self.device)

            # feed into bert
            ent_bert_encode_layers = None
            if self.training:
                self.bert.train()
                ent_bert_encode_layers, _ = self.bert(ent_x_in, ent_segment, ent_mask)
            else:
                with torch.no_grad():
                    ent_bert_encode_layers, _ = self.bert(ent_x_in, ent_segment, ent_mask)

            # concat last 4 layers output
            ent_h = torch.cat(ent_bert_encode_layers[-4:], 2) # (x_len, 4*768)
            ent_cls = ent_h[:, 0]

            # trigger prediction
            ent_logits = self.ae_ol(ent_cls)

            # 添加到 batch
            batch_ent_logits.extend(ent_logits)

        if len(batch_ent_logits) != 0:
            batch_ent_logits = torch.stack(batch_ent_logits, dim=0)
            
        return batch_trigger_logits, batch_ent_logits, batch_ent_keys

    def calculate_loss_ed(self, label, pred, words, weight):
        '''
        Calculate loss for a batched output of ed

        :param logits: FloatTensor, (batch_size, seq_len, output_class)
        :param mask: ByteTensor, mask of padded batched input sequence, (batch_size, seq_len)
        :param label: LongTensor, golden label of paadded sequences, (batch_size, seq_len)
        :return: Float, accumulated loss and index
        '''
        seq_lens = [len(w["_LIST_"]) for w in words]
        y = []
        for i, l in enumerate(seq_lens):
            y.extend(label[i][:l])
        y = torch.stack(y, dim=0)
        
        if weight is not None:
            weight = weight.to(self.device)
            loss = F.nll_loss(F.log_softmax(pred, dim=1), y, weight=weight)
        else:
            loss = F.nll_loss(F.log_softmax(pred, dim=1), y)
            
        return loss, y.tolist()

    def calculate_loss_ae(self, golden_events, pred, keys, weight):
        '''
        Calculate loss for a batched output of ae

        :param logits: FloatTensor, (N, output_class)
        :param keys: [{ batch, trigger, trigger_type, entity }, ...]
        :param batch_golden_events:
        [
            {
                (2, 3, "event_type_str") --> [(1, 2, XX), ...]
                , ...
            }, ...
        ]
        :param BATCH_SIZE: int
        :return:
            loss: Float, accumulated loss and index
            predicted_events:
            [
                {
                    (2, 3, "event_type_str") --> [(1, 2, XX), ...]
                    , ...
                }, ...
            ]
        '''
        # print(batch_golden_events)
        golden_labels = []
        # keys 是所有预测出来的argument
        for k in keys:
            label = consts.ROLE_O_LABEL
            
            batch = k["batch"]
            trigger_index = k["trigger"]
            trigger_type = k["trigger_type"]
            entity = k["entity"]
            
            # 如果预测出来的在golden event里
            if (trigger_index, trigger_type) in golden_events[batch]:  # if event matched
                golden_ents = golden_events[batch][(trigger_index, trigger_type)]
                # 找到对应label
                for ent in golden_ents:
                    if entity in ent:
                        label = ent[-1]
                        break
            golden_labels.append(label)
            
        golden_labels = torch.LongTensor(golden_labels).to(self.device)
        if weight is not None:
            weight = weight.to(self.device)
            loss = F.nll_loss(F.log_softmax(pred, dim=1), golden_labels, weight=weight)
        else:
            loss = F.nll_loss(F.log_softmax(pred, dim=1), golden_labels)

        # 将预测的结果结构化
        predicted_events = [{} for _ in range(len(golden_events))]
        output_ae = torch.max(pred, 1)[1].tolist()
        for k, ae_label in zip(keys, output_ae):
            batch = k["batch"]
            trigger_index = k["trigger"]
            trigger_type = k["trigger_type"]
            entity = k["entity"]
            
            if ae_label == consts.ROLE_O_LABEL:
                continue
                
            if (trigger_index, trigger_type) not in predicted_events[batch]:
                predicted_events[batch][(trigger_index, trigger_type)] = []

            predicted_events[batch][(trigger_index, trigger_type)].append((entity, ae_label))

        return loss, predicted_events, golden_labels.tolist()
